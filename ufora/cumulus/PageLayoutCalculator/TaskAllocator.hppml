/***************************************************************************
    Copyright 2015 Ufora Inc.
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/
#pragma once

#include "../../core/containers/MapWithIndex.hpp"
#include "../../core/containers/ImmutableTreeMap.hppml"
#include "PageMachineMap.hppml"
#include "../../core/math/Smallest.hpp"
#include "../../core/Logging.hpp"
#include "MachinePageAllocation.hppml"
#include "WorkingSet.hppml"
#include "TaskCosts.hppml"
#include "TaskAllocatorId.hppml"


namespace Cumulus {

namespace PageLayoutCalculator {

class TaskAllocator {
public:
	TaskAllocator(int64_t maxBytesToLoadAtOnce) : 
			mMaxBytesToLoadAtOnce(maxBytesToLoadAtOnce),
			mMaxFractionOfBoxToTasks(.4),
			mTotalRAM(0)
		{
		}

	void setMachineStats(MachineId machine, int64_t bytesAvailable, int64_t coreCount)
		{
		mMachines.insert(machine);
		
		mTotalRAM += bytesAvailable - mMachineSizes[machine];

		mMachineSizes[machine] = bytesAvailable;

		mMachineCoreCounts[machine] = coreCount;

		mWorkingSetPages.setMemory(machine, bytesAvailable);

		mTaskCosts.touch(machine);
		}

	int64_t totalBytes()
		{
		int64_t res = 0;

		for (auto machineAndBytes: mMachineSizes)
			res += machineAndBytes.second;

		return res;
		}

	ImmutableTreeMap<MachineId, int64_t> accumulatorBytecounts(TaskAllocatorId taskId)
		{
		ImmutableTreeMap<MachineId, int64_t> allocation;
		for (auto machineAndBytecount: mAccumulatorAllocatedBytecounts[taskId])
			allocation = allocation + machineAndBytecount;
		return allocation;
		}

	void addToWorkingSet(Fora::PageId page)
		{
		lassert(mDropped.find(page) == mDropped.end());

		if (mWorkingSet.find(page) != mWorkingSet.end())
			return;

		mWorkingSet.insert(page);

		mWorkingSetPendingAdd.insert(page);
		}

	void dropFromWorkingSet(Fora::PageId page)
		{
		mWorkingSetPendingAdd.erase(page);

		mWorkingSet.erase(page);

		mWorkingSetPages.dropPage(page);
		}

	void pageAdded(MachineId machine, Fora::PageId page)
		{
		lassert(mDropped.find(page) == mDropped.end());
		
		bool wasTarget = mPageAllocation.getTargetPages().contains(machine, page);

		mPageAllocation.pageAdded(machine, page);

		bool isTarget = mPageAllocation.getTargetPages().contains(machine, page);

		if (!wasTarget && isTarget)
			mTaskCosts.machineAddPage(machine, page);
		if (wasTarget && !isTarget)
			mTaskCosts.machineDropPage(machine, page);
		}

	void pageDropped(MachineId machine, Fora::PageId page)
		{
		lassert(mDropped.find(page) == mDropped.end());
		
		bool wasTarget = mPageAllocation.getTargetPages().contains(machine, page);

		mPageAllocation.pageDropped(machine, page);

		bool isTarget = mPageAllocation.getTargetPages().contains(machine, page);

		if (!wasTarget && isTarget)
			mTaskCosts.machineAddPage(machine, page);
		if (wasTarget && !isTarget)
			mTaskCosts.machineDropPage(machine, page);
		}

	void pageDroppedAcrossEntireSystem(Fora::PageId page)
		{
		std::set<TaskAllocatorId> tasks = mTaskCosts.taskPages().getKeys(page);
		for (auto t: tasks)
			mTaskCosts.taskDropPage(t, page);

		mPageAllocation.pageDroppedAcrossEntireSystem(page);
		mWorkingSet.erase(page);
		mWorkingSetPages.dropPage(page);

		mDropped.insert(page);
		}

	bool hasTask(TaskAllocatorId taskId) const
		{
		return mTaskThreads.find(taskId) != mTaskThreads.end();
		}

	void dropTask(TaskAllocatorId taskId)
		{
		if (taskId.type() != TaskType::Accumulator)
			{
			mTaskCosts.dropTask(taskId);
			mTaskThreads.erase(taskId);
			}
		else
			{
			mAccumulatorAllocatedBytecounts.erase(taskId);
			mAccumulatorTargetBytecounts.erase(taskId);
			mAccumulatorDesiredMachine.erase(taskId);
			}

		mKnownTasks.erase(taskId);
		mLoadsPerTask.erase(taskId);
		}

	void taskAddPage(TaskAllocatorId taskId, Fora::PageId page)
		{
		lassert(mDropped.find(page) == mDropped.end());
		
		lassert(taskId.type() == TaskType::ThreadGroup || 
				taskId.type() == TaskType::Temporary);
		if (mTaskCosts.taskPages().contains(taskId, page))
			return;

		mTaskCosts.taskAddPage(taskId, page);
		}

	void taskDropPage(TaskAllocatorId taskId, Fora::PageId page)
		{
		lassert(mDropped.find(page) == mDropped.end());
			
		if (!mTaskCosts.taskPages().contains(taskId, page))
			return;

		mTaskCosts.taskDropPage(taskId, page);
		}

	void setTaskThreads(TaskAllocatorId taskId, int64_t threadcount)
		{
		lassert(hasTask(taskId));

		mTaskThreads[taskId] = threadcount;
		}

	void setTaskProperties(
			TaskAllocatorId taskId, 
			int64_t threadcount, 
			const std::set<Fora::PageId>& pages
			)
		{
		lassert(taskId.type() == TaskType::ThreadGroup || taskId.type() == TaskType::Temporary);
		
		lassert(!hasTask(taskId));

		mKnownTasks.insert(taskId);
		mTaskCosts.touch(taskId);

		mTaskThreads[taskId] = threadcount;
		
		for (auto page: pages)
			taskAddPage(taskId, page);
		}

	void setAccumulatorProperties(
			TaskAllocatorId taskId, 
			int64_t bytecount,
			MachineId preferredMachine
			)
		{
		lassert(taskId.type() == TaskType::Accumulator);
		
		lassert(!hasTask(taskId));
		mKnownTasks.insert(taskId);

		mAccumulatorTargetBytecounts[taskId] = bytecount;
		mAccumulatorDesiredMachine[taskId] = preferredMachine;
		}

	void allocateTasks()
		{
		//see if we want to add a task into the "loading" tasks
		std::set<MachineId> underallocatedMachines;
		for (auto m: mMachines)
			if (machineCouldHandleMoreWork(m))
				underallocatedMachines.insert(m);

		for (auto t: mKnownTasks)
			mTaskCosts.touch(t);

		for (auto t: mKnownTasks)
			if (taskIsUnderallocated(t))
				mTaskCosts.markTaskUnderallocated(t);
			else
				mTaskCosts.markTaskFullyAllocated(t);

		while (true)
			{
			Smallest<pair<TaskAllocatorId, MachineId> > bestAllocation;

			std::set<MachineId> toDrop;

			for (auto m: underallocatedMachines)
				{
				if (machineCouldHandleMoreWork(m))
					{
					const auto& taskScores = mTaskCosts.allocatableTasksForMachine(m);

					if (taskScores.size())
						{
						TaskAllocatorId task = *taskScores.getKeys(taskScores.lowestValue()).begin();

						if (machineCouldLoadTask(m, task))
							bestAllocation.observe(make_pair(task, m), taskScores.lowestValue());
						}
					}
				else
					toDrop.insert(m);
				}

			for (auto m: toDrop)
				underallocatedMachines.erase(m);

			if (!bestAllocation.smallest())
				return;

			TaskAllocatorId task = bestAllocation.smallest()->first;
			MachineId machine = bestAllocation.smallest()->second;

			mLoadsPerTask[task] += bytesToScheduleTaskOnMachine(task, machine);
			mTaskCosts.addTaskLoading(task, machine);

			if (!taskIsUnderallocated(task))
				mTaskCosts.markTaskFullyAllocated(task);
			}
		}

	int64_t bytesLoadedByTaskOverLifetime(TaskAllocatorId task) const
		{
		auto it = mLoadsPerTask.find(task);
		if (it == mLoadsPerTask.end())
			return 0;
		return it->second;
		}

	void rebalance()
		{
		dropBadLoadingOrRunningTasks();

		allocateAccumulators();

		dropTasksOnOverloadedMachines();

		allocateTasks();

		rebalanceWorkingSetPages();

		assignLoads();

		dropUnusedPages();

		moveLoadingTasksToWorkingTasks();

		validateInternalInvariants();
		}

	void dropBadLoadingOrRunningTasks()
		{
		std::set<pair<TaskAllocatorId, MachineId> > badRunningTasks;
		std::set<pair<TaskAllocatorId, MachineId> > badLoadingTasks;

		for (auto machine: mMachines)
			{
			for (auto task: mTaskCosts.runningTasks().getKeys(machine))
				for (auto page: mTaskCosts.taskPages().getValues(task))
					if (!mPageAllocation.getPages().contains(machine, page))
						badRunningTasks.insert(make_pair(task, machine));

			for (auto task: mTaskCosts.loadingTasks().getKeys(machine))
				for (auto page: mTaskCosts.taskPages().getValues(task))
					if (!mPageAllocation.getTargetPages().contains(machine, page))
						badLoadingTasks.insert(make_pair(task, machine));
			}

		for (auto t: badLoadingTasks)
			mTaskCosts.dropTaskLoading(t.first, t.second);

		for (auto t: badRunningTasks)
			mTaskCosts.dropTaskRunning(t.first, t.second);
		}

	void dropTasksOnOverloadedMachines()
		{
		for (auto machine: mMachines)
			{
			while (bytesInAllTasks(machine) + bytesAllocatedToAccumulatorTasksOnMachine(machine) > 
					machineMaxMemory(machine) * mMaxFractionOfBoxToTasks && 
						(mTaskCosts.loadingTasks().getKeys(machine).size() || mTaskCosts.runningTasks().getKeys(machine).size()))
				{
				if (mTaskCosts.loadingTasks().getKeys(machine).size())
					mTaskCosts.dropTaskLoading(*mTaskCosts.loadingTasks().getKeys(machine).begin(), machine);
					else
				if (mTaskCosts.runningTasks().getKeys(machine).size())
					mTaskCosts.dropTaskRunning(*mTaskCosts.runningTasks().getKeys(machine).begin(), machine);
				}
			}
		}

	void allocateAccumulators()
		{
		for (auto& taskIdAndBytecount: mAccumulatorTargetBytecounts)
			if (accumulatorTaskIsUnderallocated(taskIdAndBytecount.first))
				tryToIncreaseAccumulatorTaskAllocation(taskIdAndBytecount.first);
		}

	bool accumulatorTaskIsUnderallocated(TaskAllocatorId task) const
		{
		return bytesAllocatedToAccumulatorTask(task) < bytesRequestedForAccumulatorTask(task);
		}

	int64_t bytesRequestedForAccumulatorTask(TaskAllocatorId task) const
		{
		auto it = mAccumulatorTargetBytecounts.find(task);
		
		if (it != mAccumulatorTargetBytecounts.end())
			return it->second;

		return 0;
		}

	int64_t bytesAllocatedToAccumulatorTask(TaskAllocatorId task) const
		{
		auto it = mAccumulatorAllocatedBytecounts.find(task);
		if (it == mAccumulatorAllocatedBytecounts.end())
			return 0;

		int64_t total = 0;
		
		for (auto machineAndBytes: it->second)
			total += machineAndBytes.second;

		return total;
		}

	int64_t bytesAllocatedToAccumulatorTaskOnMachine(TaskAllocatorId task, MachineId m) const
		{
		auto it = mAccumulatorAllocatedBytecounts.find(task);
		if (it == mAccumulatorAllocatedBytecounts.end())
			return 0;

		auto it2 = it->second.find(m);
		if (it2 == it->second.end())
			return 0;

		return it2->second;
		}

	int64_t bytesAllocatedToAccumulatorTasksOnMachine(MachineId m) const
		{
		int64_t total = 0;

		for (const auto& taskAndMachinesMap: mAccumulatorTargetBytecounts)
			total += bytesAllocatedToAccumulatorTaskOnMachine(taskAndMachinesMap.first, m);

		return total;
		}

	int64_t bytesAvailableForAccumulatorTasksOnMachine(MachineId m) const
		{
		return std::max<int64_t>(0, 
			machineMaxMemory(m) * mMaxFractionOfBoxToTasks - 
				bytesAllocatedToAccumulatorTasksOnMachine(m)
			);
		}

	void tryToAllocateNewAccumulatorTask(TaskAllocatorId task)
		{
		auto it = mAccumulatorDesiredMachine.find(task);
		lassert(it != mAccumulatorDesiredMachine.end());

		MachineId preferred = it->second;

		if (tryToAllocateAccumulatorTaskToSingleMachine(task, preferred))
			return;

		for (auto m: mMachines)
			if (tryToAllocateAccumulatorTaskToSingleMachine(task, m))
				return;

		//OK, now try to break it up across a bunch of machines. 
		//Look at all the machines and see how much space we have

		std::vector<pair<int64_t, MachineId> > bytesAvailable;
		for (auto m: mMachines)
			bytesAvailable.push_back(make_pair(-bytesAvailableForAccumulatorTasksOnMachine(m), m));

		std::sort(bytesAvailable.begin(), bytesAvailable.end());

		int64_t needed = bytesRequestedForAccumulatorTask(task);

		int64_t minBoxSize = machineMaxMemory(bytesAvailable[0].second) * mMaxFractionOfBoxToTasks;

		Nullable<int64_t> bestCount;

		for (long k = 0; k < bytesAvailable.size(); k++)
			{
			minBoxSize = std::min<int64_t>(
				minBoxSize, 
				machineMaxMemory(bytesAvailable[0].second) * mMaxFractionOfBoxToTasks
				);
		
			int64_t machinesAtThisLevel = k + 1;
			int64_t bytesToUsePerMachine = bytesAvailableForAccumulatorTasksOnMachine(bytesAvailable[k].second);

			int64_t machinesRequired = (needed - 1) / (bytesToUsePerMachine) + 1;

			if (machinesRequired <= machinesAtThisLevel)
				{
				//this is possible
				//also make sure we're using at least 10% of each box
				if (bytesToUsePerMachine > minBoxSize / 10)
					bestCount = k;
				}
			}

		if (bestCount)
			{
			int64_t bytesToUsePerMachine = (needed - 1) / (*bestCount + 1) + 1;

			for (long k = 0; k <= *bestCount; k++)
				mAccumulatorAllocatedBytecounts[task][bytesAvailable[k].second] = bytesToUsePerMachine;
			}		
		}

	void tryToIncreaseAccumulatorTaskAllocation(TaskAllocatorId task)
		{
		if (bytesAllocatedToAccumulatorTask(task) == 0)
			{
			tryToAllocateNewAccumulatorTask(task);
			return;
			}

		//see if we can meet the requirement by grossing up existing machines equally
		int64_t bytesNeeded = bytesRequestedForAccumulatorTask(task) - bytesAllocatedToAccumulatorTask(task);

			{
			Smallest<MachineId> minCount;

			for (auto machineAndBytes: mAccumulatorAllocatedBytecounts[task])
				if (bytesAvailableForAccumulatorTasksOnMachine(machineAndBytes.first) > 0)
					minCount.observe(
						machineAndBytes.first,
						bytesAvailableForAccumulatorTasksOnMachine(machineAndBytes.first)
						);

			if (minCount.smallest())
				{
				int64_t machineCount = mAccumulatorAllocatedBytecounts[task].size();

				int64_t bytesToAdd = 
					std::max<int64_t>(
						(bytesNeeded - 1) / machineCount + 1,
						*minCount.smallestWeight()
						);

				for (auto& machineAndBytes: mAccumulatorAllocatedBytecounts[task])
					{
					machineAndBytes.second += bytesToAdd;
					bytesNeeded -= bytesToAdd;
					}

				if (bytesNeeded <= 0)
					return;
				}
			}

		//look at the minimum number of bytes allocated to a box
		Smallest<MachineId> minCount;
		for (auto machineAndBytes: mAccumulatorAllocatedBytecounts[task])
			minCount.observe(machineAndBytes.first, machineAndBytes.second);

		std::set<MachineId> boxesWeCouldAdd;

		int64_t bytesToAdd = *minCount.smallestWeight();

		for (auto m: mMachines)
			if (bytesAllocatedToAccumulatorTaskOnMachine(task, m) == 0 && 
					bytesAvailableForAccumulatorTasksOnMachine(m) >= bytesToAdd)
				boxesWeCouldAdd.insert(m);

		if (boxesWeCouldAdd.size() * bytesToAdd >= bytesNeeded)
			{
			for (auto m: boxesWeCouldAdd)
				{
				mAccumulatorAllocatedBytecounts[task][m] += bytesToAdd;
				bytesNeeded -= bytesToAdd;
				if (bytesNeeded <= 0)
					return;
				}
			}

		LOG_CRITICAL << "Couldn't increase allocation for accumulator task" << task.guid();
		}

	bool tryToAllocateAccumulatorTaskToSingleMachine(TaskAllocatorId task, MachineId machine)
		{
		int64_t needed = bytesRequestedForAccumulatorTask(task);

		int64_t maxBytes = machineMaxMemory(machine) * mMaxFractionOfBoxToTasks;

		if (needed < maxBytes / 2 && needed + bytesAllocatedToAccumulatorTasksOnMachine(machine) < maxBytes)
			//we can just allocated directly
			{
			mAccumulatorAllocatedBytecounts[task][machine] = needed;
			return true;
			}

		return false;
		}

	bool machineHasTooManyPages(MachineId m) const
		{
		return mPageAllocation.bytesTargetedOnMachine(m) + 
					bytesAllocatedToAccumulatorTasksOnMachine(m) > machineMaxMemory(m);
		}

	void dropUnusedPages()
		{
		for (auto m: mMachines)
			if (machineHasTooManyPages(m))
				{
				std::set<Fora::PageId> nonWSP;
				std::set<Fora::PageId> nonTask;
				for (auto p: mPageAllocation.getTargetPages().getValues(m))
					if (mWorkingSet.find(p) == mWorkingSet.end())
						nonWSP.insert(p);
						else
					if (!isAlreadyATaskPage(m, p) && !mWorkingSetPages.getPages().contains(m, p))
						nonTask.insert(p);

				while (machineHasTooManyPages(m) && nonWSP.size())
					{
					Fora::PageId pageToDrop = *nonWSP.begin();

					nonWSP.erase(pageToDrop);
					nonTask.erase(pageToDrop);
					if (!mPageAllocation.pageIsLoading(m,pageToDrop))
						requestPageDrop(m, pageToDrop);
					}

				while (machineHasTooManyPages(m) && nonTask.size())
					{
					Fora::PageId pageToDrop = *nonTask.begin();

					nonTask.erase(pageToDrop);
					if (!mPageAllocation.pageIsLoading(m,pageToDrop))
						requestPageDrop(m, pageToDrop);
					}
				}

		}

	void moveLoadingTasksToWorkingTasks()
		{
		for (auto m: mMachines)
			{
			std::set<TaskAllocatorId> activate;

			for (auto t: mTaskCosts.loadingTasks().getKeys(m))
				if (allTaskPagesLoaded(t, m))
					activate.insert(t);

			for (auto t: activate)
				{
				mTaskCosts.dropTaskLoading(t, m);
				mTaskCosts.addTaskRunning(t, m);
				}
			}
		}

	bool allTaskPagesLoaded(TaskAllocatorId task, MachineId machine)
		{
		for (auto p: mTaskCosts.taskPages().getValues(task))
			if (!mPageAllocation.getPages().contains(machine, p))
				return false;
		return true;
		}

	MachineId mostLoadedMachine() const
		{
		return *mWorkingSetPages.getBytecountRatios().getKeys(mWorkingSetPages.getBytecountRatios().highestValue()).begin();
		}

	MachineId leastLoadedMachine() const
		{
		return *mWorkingSetPages.getBytecountRatios().getKeys(mWorkingSetPages.getBytecountRatios().lowestValue()).begin();
		}
	
	void rebalanceWorkingSetPages()
		{
		if (mTotalRAM == 0)
			return;

		for (auto page: mWorkingSetPendingAdd)
			{
			Smallest<MachineId> best;
			for (auto machine: mPageAllocation.getTargetPages().getKeys(page))
				best.observe(machine, wspRatio(machine));

			if (best.smallest())
				mWorkingSetPages.add(*best.smallest(), page);
			else
				mWorkingSetPages.add(leastLoadedMachine(), page);
			}

		mWorkingSetPendingAdd.clear();

		double idealWSFraction = mWorkingSetPages.totalBytecount() / double(mTotalRAM);

		while (wspRatio(mostLoadedMachine()) > idealWSFraction + .1)
			{
			reallocateWorkingSetPagesAway(
				mostLoadedMachine(), 
				idealWSFraction + .05, 
				false
				);
			reallocateWorkingSetPagesAway(
				mostLoadedMachine(), 
				idealWSFraction + .05, 
				true
				);
			}

		if (idealWSFraction > .1)
			while (wspRatio(leastLoadedMachine()) < idealWSFraction / 3)
				reallocateOneWorkingSetPageTowards(
					leastLoadedMachine(), 
					idealWSFraction - .05
					);
		}

	double wspRatio(MachineId m) const
		{
		return mWorkingSetPages.bytecountRatioFor(m);
		}

	void reallocateOneWorkingSetPageTowards(MachineId machine, double targetFraction)
		{
		if (wspRatio(machine) > targetFraction || machine == mostLoadedMachine())
			return;

		if (mWorkingSetPages.getPages().getValues(mostLoadedMachine()).size() 
				&& wspRatio(machine) < targetFraction && mostLoadedMachine() != machine)
			{
			Fora::PageId p = *mWorkingSetPages.getPages().getValues(mostLoadedMachine()).begin();

			mWorkingSetPages.dropPage(p);
			mWorkingSetPages.add(machine, p);
			}
		}

	void reallocateWorkingSetPagesAway(MachineId machine, double targetFraction, bool allowMove)
		{
		if (wspRatio(machine) < targetFraction)
			return;

		double idealWsRatio = mWorkingSetPages.totalBytecount() / double(mTotalRAM) + .05;

		if (allowMove)
			{
			while (mWorkingSetPages.getPages().getValues(machine).size() && wspRatio(machine) > targetFraction)
				{
				Fora::PageId p = *mWorkingSetPages.getPages().getValues(machine).begin();

				MachineId leastLoaded = leastLoadedMachine();

				if (leastLoaded == machine)
					return;

				mWorkingSetPages.dropPage(p);
				mWorkingSetPages.add(leastLoaded, p);
				}
			}
		else
			{
			MapWithIndex<Fora::PageId, pair<double, MachineId> > fractions;

			for (auto p: mWorkingSetPages.getPages().getValues(machine))
				{
				Smallest<MachineId> best;
				
				for (auto targetM: mPageAllocation.getTargetPages().getKeys(p))
					best.observe(targetM, wspRatio(targetM));

				if (best.smallestWeight())
					fractions.set(p, make_pair(*best.smallestWeight(), *best.smallest()));
				}

			for (auto& fracMachineAndPage: fractions.getValueToKeys())
				if (wspRatio(machine) > targetFraction)
					{
					for (auto p: fracMachineAndPage.second)
						{
						Smallest<MachineId> best;
						
						for (auto targetM: mPageAllocation.getTargetPages().getKeys(p))
							best.observe(targetM, wspRatio(targetM));

						if (best.smallestWeight() && *best.smallestWeight() < idealWsRatio)
							{
							mWorkingSetPages.dropPage(p);
							mWorkingSetPages.add(*best.smallest(), p);
							}
						}
					}
				else
					return;
			}
		}

	void assignLoads()
		{
		for (auto m: mMachines)
			if (mPageAllocation.bytesAddedToMachine(m) < mMaxBytesToLoadAtOnce)
				{
				for (auto wsp: mWorkingSetPages.getPages().getValues(m))
					if (mPageAllocation.bytesAddedToMachine(m) < mMaxBytesToLoadAtOnce)
						{
						if (!mPageAllocation.getTargetPages().contains(m, wsp))
							requestPageAdd(m, wsp);
						}

				for (auto loadingTask: mTaskCosts.loadingTasks().getKeys(m))
					for (auto p: mTaskCosts.taskPages().getValues(loadingTask))
						if (mPageAllocation.bytesAddedToMachine(m) < mMaxBytesToLoadAtOnce)
							if (!mPageAllocation.getTargetPages().contains(m, p))
								requestPageAdd(m, p);
				}
		}

	int64_t bytesToScheduleTaskOnMachine(TaskAllocatorId task, MachineId machine) const
		{
		const auto& costs = mTaskCosts.taskCosts(task);
		if (costs.hasKey(machine))
			return costs.getValue(machine);
		return 0;
		}

	bool machineCouldLoadTask(MachineId machine, TaskAllocatorId newTask) const
		{
		int64_t bytes = 0;

		if (bytesInLoadingTasks(machine) > mMaxBytesToLoadAtOnce)
			return false;

		for (auto p: mTaskCosts.taskPages().getValues(newTask))
			if (!isAlreadyATaskPage(machine, p))
				bytes += p.bytecount();

		if (bytesAllocatedToAccumulatorTasksOnMachine(machine) + 
				bytesInAllTasks(machine) + bytes > machineMaxMemory(machine) * mMaxFractionOfBoxToTasks)
			return false;

		return true;
		}

	bool isAlreadyATaskPage(MachineId machine, Fora::PageId page) const
		{
		for (auto task: mTaskCosts.runningTasks().getKeys(machine))
			if (mTaskCosts.taskPages().contains(task, page))
				return true;

		for (auto task: mTaskCosts.loadingTasks().getKeys(machine))
			if (mTaskCosts.taskPages().contains(task, page))
				return true;

		return false;
		}

	int64_t bytesInLoadingTasks(MachineId machine) const
		{
		std::set<Fora::PageId> usedPages;
		int64_t bytes = 0;

		for (auto task: mTaskCosts.loadingTasks().getKeys(machine))
			for (auto p: mTaskCosts.taskPages().getValues(task))
				if (usedPages.find(p) == usedPages.end())
					{
					usedPages.insert(p);
					bytes += p.bytecount();
					}

		return bytes;
		}

	int64_t bytesInAllTasks(MachineId machine) const
		{
		std::set<Fora::PageId> usedPages;
		int64_t bytes = 0;

		for (auto task: mTaskCosts.runningTasks().getKeys(machine))
			for (auto p: mTaskCosts.taskPages().getValues(task))
				if (usedPages.find(p) == usedPages.end())
					{
					usedPages.insert(p);
					bytes += p.bytecount();
					}

		for (auto task: mTaskCosts.loadingTasks().getKeys(machine))
			for (auto p: mTaskCosts.taskPages().getValues(task))
				if (usedPages.find(p) == usedPages.end())
					{
					usedPages.insert(p);
					bytes += p.bytecount();
					}

		return bytes;
		}

	bool taskIsUnderallocated(TaskAllocatorId task) const
		{
		int64_t totalCores = 0;
		for (auto machine: mTaskCosts.runningTasks().getValues(task))
			totalCores += machineTaskCores(machine, task);

		for (auto machine: mTaskCosts.loadingTasks().getValues(task))
			totalCores += machineTaskCores(machine, task);

		if (totalCores * 1.5 < taskThreads(task))
			return true;

		return false;
		}

	bool machineCouldHandleMoreWork(MachineId m) const
		{
		return machineMaxCoreCount(m) > machineAllocatedCoreCount(m);
		}

	const std::set<MachineId>& machines() const
		{
		return mMachines;
		}

	int64_t machineMaxMemory(MachineId m) const
		{
		auto it = mMachineSizes.find(m);
		if (it == mMachineSizes.end())
			return 0;
		return it->second;
		}

	int64_t machineAllocatedCoreCount(MachineId m) const
		{
		float cores = 0;
		
		for (auto t: mTaskCosts.runningTasks().getKeys(m))
			cores += machineTaskCores(m, t);

		return std::ceil(cores);
		}

	int64_t machineTaskCores(MachineId m, TaskAllocatorId t) const
		{
		int64_t liveMachines = mTaskCosts.runningTasks().getValues(t).size() + mTaskCosts.loadingTasks().getValues(t).size();
		if (liveMachines == 0)
			return 0;

		return std::min<int64_t>(
			std::max<int64_t>(
				taskThreads(t) ? 1 : 0, 
				taskThreads(t) / liveMachines
				),
			machineMaxCoreCount(m)
			);
		}

	int64_t taskThreads(TaskAllocatorId task) const
		{
		auto it = mTaskThreads.find(task);
		
		if (it != mTaskThreads.end())
			return it->second;

		return 0;
		}

	int64_t machineMaxCoreCount(MachineId m) const
		{
		auto it = mMachineCoreCounts.find(m);

		if (it != mMachineCoreCounts.end())
			return it->second;

		return 0;
		}

	void requestPageAdd(MachineId machine, Fora::PageId page)
		{
		lassert(mDropped.find(page) == mDropped.end());
		
		bool wasTarget = mPageAllocation.getTargetPages().contains(machine, page);

		mPageAllocation.requestPageAdd(machine, page);

		bool isTarget = mPageAllocation.getTargetPages().contains(machine, page);

		if (!wasTarget && isTarget)
			mTaskCosts.machineAddPage(machine, page);
		if (wasTarget && !isTarget)
			mTaskCosts.machineDropPage(machine, page);
		}

	void requestPageDrop(MachineId machine, Fora::PageId page)
		{
		lassert(mDropped.find(page) == mDropped.end());
		
		lassert_dump(
			!mPageAllocation.isPageLoading(machine, page),
			"illegal to drop a page that we're trying to load. Wait until the load is acknowledged."
			);
		bool wasTarget = mPageAllocation.getTargetPages().contains(machine, page);

		mPageAllocation.requestPageDrop(machine, page);

		bool isTarget = mPageAllocation.getTargetPages().contains(machine, page);

		if (!wasTarget && isTarget)
			mTaskCosts.machineAddPage(machine, page);
		if (wasTarget && !isTarget)
			mTaskCosts.machineDropPage(machine, page);
		}

	MachinePageAllocation& getPageAllocation()
		{
		return mPageAllocation;
		}

	const MachinePageAllocation& getPageAllocation() const
		{
		return mPageAllocation;
		}

	void validateInternalInvariants()
		{
		//no "running tasks" missing pages
		for (auto m: mMachines)
			for (auto t: mTaskCosts.runningTasks().getKeys(m))
				for (auto p: mTaskCosts.taskPages().getValues(t))
					lassert(mPageAllocation.getPages().contains(m,p));
		}

	const TwoWaySetMap<TaskAllocatorId, MachineId>& loadingTasks() const
		{
		return mTaskCosts.loadingTasks();
		}

	const TwoWaySetMap<TaskAllocatorId, MachineId>& runningTasks() const
		{
		return mTaskCosts.runningTasks();
		}

private:
	TaskCosts mTaskCosts;

	double mMaxFractionOfBoxToTasks;

	map<TaskAllocatorId, int64_t> mLoadsPerTask;

	//each WS page has a particular machine it belongs to
	PageMachineMap mWorkingSetPages;

	//where all the pages are
	MachinePageAllocation mPageAllocation;

	std::set<MachineId> mMachines;

	map<MachineId, int64_t> mMachineSizes;

	map<MachineId, int64_t> mMachineCoreCounts;

	std::set<Fora::PageId> mDropped;

	//all pages in the Working set
	std::set<Fora::PageId> mWorkingSet;

	std::set<Fora::PageId> mWorkingSetPendingAdd;
	
	int64_t mMaxBytesToLoadAtOnce;

	int64_t mTotalRAM;

	//task state
	std::set<TaskAllocatorId> mKnownTasks;

	std::map<TaskAllocatorId, int64_t> mTaskThreads;

	std::map<TaskAllocatorId, int64_t> mAccumulatorTargetBytecounts;

	std::map<TaskAllocatorId, MachineId> mAccumulatorDesiredMachine;

	std::map<TaskAllocatorId, map<MachineId, int64_t> > mAccumulatorAllocatedBytecounts;
};

}

}