/***************************************************************************
   Copyright 2015 Ufora Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
****************************************************************************/
#include "LocalSchedulerImplKernel.hppml"

#include "../CumulusComponentMessageCreated.hppml"
#include "../InitiateComputationMove.hppml"
#include "../SystemwidePageRefcountTracker.hppml"
#include "../../core/Clock.hpp"
#include "../../core/Logging.hpp"
#include "../../core/threading/CallbackScheduler.hppml"
#include "../../core/threading/Queue.hpp"
#include "../../FORA/TypedFora/ABI/BigVectorLayouts.hppml"

#include <iomanip>

class SystemwidePageRefcountTrackerEvent;

//using namespace Cumulus::PageLayoutCalculator;

namespace Cumulus {
namespace SystemwideComputationScheduler {

const double kTimeBetweenPageResets = 5.0;
const double kMaxLoadToFillACpu = .5;
const double kBroadcastInterval = .1;

LocalSchedulerImplKernel::LocalSchedulerImplKernel(
			uint64_t vdmMaxPageSizeInBytes,
			uint64_t vdmMemoryLimitInBytes,
        	MachineId inOwnMachineId,
			long inActiveThreadCount,
			boost::function1<void, InitiateComputationMove> onInitiateComputationMoved,
			boost::function1<void, CumulusComponentMessageCreated> onCumulusComponentMessageCreated
			) : 
		mOwnMachineId(inOwnMachineId),
		mOnInitiateComputationMoved(onInitiateComputationMoved),
		mOnCumulusComponentMessageCreated(onCumulusComponentMessageCreated),
		mActiveThreadCount(inActiveThreadCount),
		mRandomGenerator(inOwnMachineId.guid()[0] + inOwnMachineId.guid()[1]),
		mInitializationParameters(
			vdmMaxPageSizeInBytes,
			vdmMemoryLimitInBytes,
			inOwnMachineId,
			inActiveThreadCount
			),
		mComputationsMoved(0),
		mLastDumpTime(0),
		mMachineLoads(inOwnMachineId, inActiveThreadCount),
		mTimesCalledSinceLastDump(0),
		mLastBroadcast(0),
		mLastBroadcastLoad(0),
		mTotalMoveAttempts(0),
		mTotalSplitAttempts(0),
		mCalculationsCompleted(0),
		mThreadGroupStatusTracker(
			boost::bind(&LocalSchedulerImplKernel::sendLocalToLocalSchedulerMessage, this, boost::arg<1>()),
			boost::bind(&LocalSchedulerImplKernel::sendLocalToGlobalSchedulerMessage, this, boost::arg<1>()),
			mMachineHashTable,
			inOwnMachineId
			),
		mRuntimePredictionModel(
			boost::bind(&LocalSchedulerImplKernel::sendLocalToLocalSchedulerMessage, this, boost::arg<1>()),
			mMachineHashTable,
			inOwnMachineId
			)
	{
	mCurrentMachines.insert(mOwnMachineId);
	mMachineHashTable.addMachine(mOwnMachineId);
	}

LocalSchedulerImplKernel::~LocalSchedulerImplKernel()
	{
	logCurrentState_();
	}

void LocalSchedulerImplKernel::sendSchedulerToComputationMessage(SchedulerToComputationMessage msg)
	{
	mOnCumulusComponentMessageCreated(
		CumulusComponentMessageCreated(
			CumulusComponentMessage::SchedulerToComputation(msg),
			CumulusComponentEndpointSet::SpecificWorker(msg.computationMachine()),
			CumulusComponentType::ActiveComputations()
			)
		);
	}

void LocalSchedulerImplKernel::sendLocalToGlobalSchedulerMessage(LocalToGlobalSchedulerMessage msg)
	{
	mOnCumulusComponentMessageCreated(
		CumulusComponentMessageCreated(
			CumulusComponentMessage::LocalToGlobalScheduler(msg),
			CumulusComponentEndpointSet::LeaderMachine(),
			CumulusComponentType::GlobalScheduler()
			)
		);
	}

void LocalSchedulerImplKernel::sendLocalToLocalSchedulerMessage(LocalToLocalSchedulerMessage msg)
	{
	mOnCumulusComponentMessageCreated(
		CumulusComponentMessageCreated(
			CumulusComponentMessage::LocalToLocalScheduler(msg),
			CumulusComponentEndpointSet::SpecificWorker(msg.destMachine()),
			CumulusComponentType::LocalScheduler()
			)
		);
	}

void LocalSchedulerImplKernel::pageNoLongerReferencedAcrossSystem(Fora::PageId page)
	{
	LOG_DEBUG << "On " << prettyPrintString(mOwnMachineId) 
		<< ", page " << prettyPrintString(page) << " no longer referenced across system.";

	mThreadGroupStatusTracker.pageNoLongerReferencedAcrossSystem(page);
	}

void LocalSchedulerImplKernel::addMachine(MachineId inMachine)
	{
	mCurrentMachines.insert(inMachine);
	
	mMachineHashTable.addMachine(inMachine);
	
	//make sure there's a processor load entry
	mMachineLoads.addMachine(inMachine);
	}

void LocalSchedulerImplKernel::computationComputeStatusChanged(
											const ComputationComputeStatusChanged& change
											)
	{
	if (change.isComputingNow())
		mCurrentlyComputingComputations.insert(change.computation());
	else
		mCurrentlyComputingComputations.erase(change.computation());
	}

void LocalSchedulerImplKernel::computationStatusChanged(
								const LocalComputationPriorityAndStatusChanged& change,
								double curTime
								)
	{
	mThreadGroupStatusTracker.handleStatusChanged(change);

	@match LocalComputationPriorityAndStatusChanged(change)
		-| Active(computation, priority, status, statistics) ->> {
			if (status.isFinished() && !mLocalComputationStatuses[change.computation()].isFinished())
				{
				mCalculationsCompleted++;
				if (statistics.initialPredictionSignature())
					mRuntimePredictionModel.handleRuntimePredictionObservation(
						RuntimePredictionObservation(
							*statistics.initialPredictionSignature(),
							statistics.estimatedTotalRuntime()
							)
						);
				}

			mLocalComputationStatuses[change.computation()] = status;
			mLocalComputationPriorities[change.computation()] = priority;

			if (priority.isCircular())
				sendSchedulerToComputationMessage(
					SchedulerToComputationMessage::MarkSelfCircular(
						computation,
						hash_type(),
						mOwnMachineId,
						mOwnMachineId
						)
					);

			if (status.isBlockedOnVectorLoad())
				mComputationsBlockedOnVectorsLocally.insert(change.computation());
			else
				mComputationsBlockedOnVectorsLocally.erase(change.computation());

			if (status.isBlockedOnComputations())
				mComputationsBlockedOnComputationsLocally.insert(change.computation());
			else
				mComputationsBlockedOnComputationsLocally.erase(change.computation());

			if (status.isFinished() && computation.isSplit())
				mSplitComputationsFinishedButUncollected.insert(computation);
			else
				mSplitComputationsFinishedButUncollected.erase(computation);

			@match ComputationStatus(status)
				-| BlockedOnComputations(computations) ->> {
					}
				-| Finished() ->> {
					mCurrentlyComputingComputationsLastPageResetTimes.erase(computation);
					mTryingToSplit.erase(computation);
					}
				-| BlockedOnVectorLoad(pages) ->> {
					sendLocalToGlobalSchedulerMessage(
						LocalToGlobalSchedulerMessage::TriggerPageLayoutRecompute()
						);

					handleComputationBlocked_(computation, statistics.pagesCurrentlyBeingUsed());
					}
				-| _ ->> {}

			if (!priority.isNull() && status.isComputable())
				computationIsComputable_(computation, priority);
			else
				computationNotComputable_(computation);


			if (statistics.pagesCurrentlyBeingUsed().size() > 10)
				sendSchedulerToComputationMessage(
					SchedulerToComputationMessage::Split(
						computation,
						hash_type(),
						mOwnMachineId,
						mOwnMachineId,
						0
						)
					);			
			}
		-| Inactive(computation) ->> {
			mLocalComputationStatuses.erase(computation);
			mLocalComputationPriorities.erase(computation);
			mComputationsBlockedOnVectorsLocally.erase(computation);
			mComputationsBlockedOnComputationsLocally.erase(computation);
			computationNotComputable_(computation);
			mTryingToSplit.erase(computation);
			mSplitComputationsFinishedButUncollected.erase(computation);
			}
	}


void LocalSchedulerImplKernel::computationIsComputable_(ComputationId computation, ComputationPriority newPriority)
	{
	if (!mCurrentlyComputableComputations.hasKey(computation) || 
			mCurrentlyComputableComputations.getValue(computation).second != 
				newPriority)
		{
		mCurrentlyComputableComputationsByPriority.set(computation, newPriority);
		mCurrentlyComputableComputations.set(
			computation, 
			make_pair(0, newPriority)
			);
		}
	}

void LocalSchedulerImplKernel::computationNotComputable_(ComputationId computation)
	{
	mMachineLoads.computationNotComputable(computation);

	if (mCurrentlyComputableComputations.hasKey(computation))
		{
		mCurrentlyComputableComputations.drop(computation);
		mCurrentlyComputableComputationsByPriority.drop(computation);
		}
	}

void LocalSchedulerImplKernel::handleComputationBlocked_(
						const ComputationId& computation, 
						ImmutableTreeSet<Fora::PageId> pages
						)
	{
	if (mMachineLoads.computationsMoving().hasKey(computation))
		{
		LOG_INFO << "not worrying about " << computation.guid()
			<< " because it's already being moved.";
		return;
		}

	if (mCurrentlyComputableComputations.hasKey(computation))
		{
		mCurrentlyComputableComputations.drop(computation);
		mCurrentlyComputableComputationsByPriority.drop(computation);
		}
	
	if (mMachineLoads.computationsMoving().hasKey(computation))
		return;

	//attempt to split this computation
	Nullable<ThreadGroup> group = mAllThreadGroups.containingThread(ThreadGroup::groupFor(computation));

	if (group)
		for (auto p: pages)
			if (mSplitPages.find(make_pair(*group, p)) == mSplitPages.end())
				{
				mSplitPages.insert(make_pair(*group, p));
				sendSchedulerToComputationMessage(
					SchedulerToComputationMessage::Split(
						computation, 
						hash_type(), 
						mOwnMachineId,
						mOwnMachineId,
						0.0
						)
					);
				}

	}

void LocalSchedulerImplKernel::logTryingToMove_(
												const ComputationId& computation,
												const MachineId& machine
												)
	{
	mComputationsMoved++;

	LOG_DEBUG << "moving " << prettyPrintString(computation) << " from " 
		<< prettyPrintString(mOwnMachineId)
		<< " to " 
		<< prettyPrintString(machine)
		;
	}

void LocalSchedulerImplKernel::handleCumulusComponentMessage(
	            CumulusComponentMessage message, 
    	        CumulusClientOrMachine source, 
        	    CumulusComponentType componentType,
        	    double curTime
        	    )
	{
	@match CumulusComponentMessage(message)
		-| ActiveComputationsToLocalScheduler(
				InitiateComputationMoveResponse(computation, success)
				) ->> {
			handleInitiateComputationMoveResponse(computation, success);
			}
		-| ActiveComputationsToLocalScheduler(
				ComputationToScheduler(msg)
				) ->> {
			handleComputationToSchedulerMessage(msg, curTime);
			}
		-| LocalToLocalSchedulerBroadcast(msg) ->> {
			handleLocalToLocalSchedulerBroadcastMessage(msg);
			}
		-| LocalToLocalScheduler(msg) ->> {
			handleLocalToLocalSchedulerMessage(msg);
			}
		-| GlobalToLocalScheduler(msg) ->> {
			handleGlobalToLocalSchedulerMessage(msg);
			}
		-| LocalComputationPriorityAndStatus(msg) ->> {
			computationStatusChanged(msg, curTime);
			}
		-| ComputationComputeStatus(msg) ->> {
			computationComputeStatusChanged(msg);
			}
	}

void LocalSchedulerImplKernel::handleInitiateComputationMoveResponse(
							const ComputationId& inComputation,
							bool isSuccess
							)
	{
	mMachineLoads.taskIsNotMoving(inComputation);
	}

void LocalSchedulerImplKernel::handleComputationToSchedulerMessage(
							const ComputationToSchedulerMessage& response,
							double curTime
							)
	{
	if (response.isEvents())
		{
		mThreadGroupStatusTracker.handleComputationEvents(
			response.computation(), 
			response.getEvents().events()
			);

		return;
		}

	if (response.guid() == hash_type())
		mTryingToSplit.erase(response.computation());
	}

Nullable<ComputationId> LocalSchedulerImplKernel::searchForSplittableComputation_()
	{
	if (!mCurrentlyComputableComputations.size())
		return null();

	bool hasRecycled = false;

	while (true)
		{
		for (auto it = mCurrentlyComputableComputations.getValueToKeys().rbegin(); 
					it != mCurrentlyComputableComputations.getValueToKeys().rend();
					++it
					)
			{
			if (it->first.first != 0)
				//we've tried to split everything in the non-recycled list. At this point,
				//all entries will have '-1' in the first part of the pair, indicating that
				//we tried them once. We should just wait for the responses to come back.
				break;

			for (auto it2 = it->second.begin(); it2 != it->second.end(); ++it2)
				{
				if (mTryingToSplit.find(*it2) == mTryingToSplit.end())
					return null() << *it2;
				}
			}

		if (!hasRecycled)
			{
			recyclePriorityList_();
			hasRecycled = true;
			}
		else
			return null();
		}

	return null();
	}

void LocalSchedulerImplKernel::recyclePriorityList_()
	{
	//reset all pairs for which the first element is less than zero.
	while (mCurrentlyComputableComputations.lowestValue().first < 0)
		{
		ComputationId id = 
			*mCurrentlyComputableComputations.getValueToKeys().begin()->second.begin();

		pair<long, ComputationPriority> priorityPair = 
			mCurrentlyComputableComputations.getValue(id);

		priorityPair.first = 0;

		mCurrentlyComputableComputations.set(id, priorityPair);
		}
	}

bool LocalSchedulerImplKernel::wantsToSplit_()
	{
	if (mComputationsBlockedOnVectorsLocally.size() > mActiveThreadCount * 2)
		return false;

	if (mMachineLoads.wantsDisableSplitDueToSystemCapacityOverload())
		return false;

	return mMachineLoads.computationsMoving().size() + 
			mCurrentlyComputableComputations.size() +
			mTryingToSplit.size() < 
				std::max<long>(mActiveThreadCount + 4, mActiveThreadCount * 2);
	}

void LocalSchedulerImplKernel::splitOrMoveIfNeeded(double curTime)
	{
	resetPageDataIfNecessary_(curTime);

	if (wantsToSplit_())
		tryToSplitSomething_();

	updateCurrentProcessorLoadAndBroadcast_(curTime);

	moveIncorrectlyScheduledTasks_();

	long passes = 0;
	while (mMachineLoads.shouldTryToMoveSomething())
		{
		passes++;
		if (!tryToMoveSomething_())
			break;
		}

	mTimesCalledSinceLastDump++;

	if (curClock() > mLastDumpTime + 2.0)
		{
		logCurrentState_();

		mTimesCalledSinceLastDump = 0;

		mLastDumpTime = curClock();
		}
	}

void LocalSchedulerImplKernel::logCurrentState_()
	{
	LOGGER_INFO_T log = LOGGER_INFO;

	map<MachineId, long> recv;
	for (const auto& mAB: mMostRecentBroadcasts)
		for (auto machineAndCount: mAB.second.moveTargets())
			recv[machineAndCount.first] += machineAndCount.second;
	

	for (auto machineAndLoad: mMachineLoads.getMachineLoads().getKeyToValue())
		{
		if (machineAndLoad.first == mOwnMachineId)
			log << "** ";
		else
			log << "   ";

		log << machineAndLoad.first << " -> " 
			<< std::setw(6)
			<< machineAndLoad.second << ". receiving " 
			<< std::setw(6)
			<< recv[machineAndLoad.first]
			<< ". "
			;

		log << "comp: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].computableComputations() << ". ";
		log << "uncomp: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].noncomputableComputations() << ". ";
		log << "mov: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].movingComputations() << ". ";
		log << "splt: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].splittingComputations() << ". ";
		log << "moveLag: " << std::setprecision(2) << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].ageOfOldestMove() << ". ";
		log << "totMov: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].totalMoves() << ". ";
		log << "totSplt: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].totalSplits() << ". ";
		log << "everFin: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].totalCompleted() << ". ";
		log << "vecload: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].blockedComputations() << ". ";
		log << "onSubcomp: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].blockedOnSubcomputations() << ". ";
		log << "finished: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].finishedButUncollected() << ". ";
		log << "wantsMove: " << (mMachineLoads.shouldMoveToMachine(machineAndLoad.first) ?"yes":"no") << ". ";

		log << "\n";
		}

	log << "Called " << mTimesCalledSinceLastDump << " since last dump.\n";
	log << mComputationsBlockedOnVectorsLocally.size() << " are blocked locally.\n";
	
	int64_t belongsOnOther = 0;
	int64_t confused = 0;

	for (auto v: mComputationsBlockedOnVectorsLocally)
		if (computationBelongsOnAnotherMachine_(v))
			belongsOnOther++;
			else
		if (!mAllThreadGroups.containingThread(ThreadGroup::groupFor(v)))
			confused++;

	log << "Computations belonging elsewhere: " << belongsOnOther << "\n";
	log << "Computations that don't know where they belong: " << confused << "\n";
	log << "Computations currently moving: " << mMachineLoads.computationsMoving().size() << "\n";

	}

void LocalSchedulerImplKernel::resetPageDataIfNecessary_(double curTime)
	{
	for (auto computation: mCurrentlyComputingComputations)
		{
		if (mCurrentlyComputingComputationsLastPageResetTimes[computation] < 
													curTime - kTimeBetweenPageResets)
			{
			sendSchedulerToComputationMessage(
				SchedulerToComputationMessage::ResetPageDataAndBroadcast(
					computation, 
					hash_type(1),
					mOwnMachineId,
					mOwnMachineId
					)
				);

			mCurrentlyComputingComputationsLastPageResetTimes[computation] = curTime;
			}
		}
	}

void LocalSchedulerImplKernel::handleLocalToLocalSchedulerBroadcastMessage(
										LocalToLocalSchedulerBroadcastMessage message
										)
	{
	@match LocalToLocalSchedulerBroadcastMessage(message)
		-| CurrentLoad() with (sourceMachine) ->> {
			mMachineLoads.setLoad(
				sourceMachine, 
				message.getCurrentLoad().expectedCpusWorthOfCompute(),
				message.getCurrentLoad().blockedComputations()
				);

			mMostRecentBroadcasts[sourceMachine] = message.getCurrentLoad();
			}
	}

void LocalSchedulerImplKernel::handleLocalToLocalSchedulerMessage(
										LocalToLocalSchedulerMessage message
										)
	{
	mThreadGroupStatusTracker.handleLocalToLocalSchedulerMessage(message);
	mRuntimePredictionModel.handleLocalToLocalSchedulerMessage(message);
	}

void LocalSchedulerImplKernel::handleGlobalToLocalSchedulerMessage(
										GlobalToLocalSchedulerMessage message
										)
	{
	@match GlobalToLocalSchedulerMessage(message)
		-| GlobalSchedulerToThreadGroup(msg) ->> {
			mThreadGroupStatusTracker.handleGlobalToLocalSchedulerMessage(msg);
			}
		-| SetActiveThreadGroups(groups) ->> {
			mAllThreadGroups.clear();

			mAllThreadGroupsActiveOn = TwoWaySetMap<ThreadGroup, MachineId>();

			for (auto machineAndGroups: groups)
				for (auto group: machineAndGroups.second)
					{
					mAllThreadGroups.insert(group);
					mAllThreadGroupsActiveOn.insert(group, machineAndGroups.first);
					}

			moveIncorrectlyScheduledTasks_();
			}
	}

void LocalSchedulerImplKernel::updateCurrentProcessorLoadAndBroadcast_(double curTime)
	{
	double total = mCurrentlyComputableComputations.size();

	long newLoadLevel = total;

	if (newLoadLevel != mMachineLoads.ownLoadRaw())
		mMachineLoads.setLoad(mOwnMachineId, newLoadLevel, mComputationsBlockedOnVectorsLocally.size());

	if (curTime - mLastBroadcast > kBroadcastInterval || 
			newLoadLevel > mActiveThreadCount && mLastBroadcastLoad < mActiveThreadCount ||
			newLoadLevel < mActiveThreadCount && mLastBroadcastLoad > mActiveThreadCount
			)
		{
		mLastBroadcast = curTime;
		mLastBroadcastLoad = newLoadLevel;

		ImmutableTreeMap<MachineId, long> sending;
		for (const auto& machineAndComps: mMachineLoads.computationsMoving().getValueToKeys())
			sending = sending + machineAndComps.first + (long)machineAndComps.second.size();

		LocalToLocalSchedulerBroadcastMessage msg = 
			LocalToLocalSchedulerBroadcastMessage::CurrentLoad(
				mOwnMachineId,
				mCurrentlyComputableComputations.size(), 
				mLocalComputationStatuses.size() - mCurrentlyComputableComputations.size(),
				newLoadLevel,
				mMachineLoads.computationsMoving().size(),
				mTryingToSplit.size(),
				mMachineLoads.ageOfOldestMove(),
				sending,
				mComputationsMoved,
				mTotalSplitAttempts,
				mCalculationsCompleted,
				mComputationsBlockedOnVectorsLocally.size(),
				mComputationsBlockedOnComputationsLocally.size(),
				mSplitComputationsFinishedButUncollected.size()
				);

		mOnCumulusComponentMessageCreated(
			CumulusComponentMessageCreated(
				CumulusComponentMessage::LocalToLocalSchedulerBroadcast(
					msg
					),
				CumulusComponentEndpointSet::AllWorkersExceptSelf(),
				CumulusComponentType::LocalScheduler()
				)
			);

		mMostRecentBroadcasts[mOwnMachineId] = msg.getCurrentLoad();
		}
	}

bool LocalSchedulerImplKernel::computationBelongsOnAnotherMachine_(ComputationId comp)
	{	
	const std::set<MachineId>& activeOn = validMachinesForComputation_(comp);

	if (activeOn.size() && activeOn.find(mOwnMachineId) == activeOn.end())
		return true;

	return false;
	}

const std::set<MachineId>& LocalSchedulerImplKernel::validMachinesForComputation_(ComputationId comp)
	{
	static std::set<MachineId> empty;

	Nullable<ThreadGroup> owningGroup = 
		mAllThreadGroups.containingThread(ThreadGroup::groupFor(comp));

	if (!owningGroup)
		return empty;

	return mAllThreadGroupsActiveOn.getValues(*owningGroup);
	}

void LocalSchedulerImplKernel::moveIncorrectlyScheduledTasks_()
	{
	//don't schedule too many computations to move
	if (mMachineLoads.computationsMoving().size() > 20)
		return;

	for (auto compAndPri: mCurrentlyComputableComputationsByPriority.getKeyToValue())
		if (computationBelongsOnAnotherMachine_(compAndPri.first) && !mMachineLoads.computationsMoving().hasKey(compAndPri.first))
			{
			const auto& machines = validMachinesForComputation_(compAndPri.first);

			tryToMoveComputationToOneOf_(compAndPri.first, machines, true);

			if (mMachineLoads.computationsMoving().size() > 20)
				return;
			}

	for (auto comp: mComputationsBlockedOnVectorsLocally)
		{	
		if (computationBelongsOnAnotherMachine_(comp) && !mMachineLoads.computationsMoving().hasKey(comp))
			{
			const auto& machines = validMachinesForComputation_(comp);

			tryToMoveComputationToOneOf_(comp, machines, true);

			if (mMachineLoads.computationsMoving().size() > 20)
				return;
			}
		}
	}

bool LocalSchedulerImplKernel::tryToMoveComputationToOneOf_(ComputationId comp, const std::set<MachineId>& activeOn, bool moveEvenIfOtherMachineIsLoaded)
	{
	if (activeOn.size())
		{
		std::vector<pair<hash_type, MachineId> > hashRing;
		for (auto machine: activeOn)
			if (machine != mOwnMachineId)
				hashRing.push_back(make_pair(machine.guid() + mOwnMachineId.guid(), machine));

		std::sort(hashRing.begin(), hashRing.end());

		std::vector<MachineId> possible;

		for (long k = 0; k < hashRing.size() && k < 5; k++)
			possible.push_back(hashRing[k].second);

		for (long k = 0; k + 1 < possible.size(); k++)
			std::swap(
				possible[k], 
				possible[k + (possible.size() - k) * mRandomGenerator()]
				);

		for (auto machine: possible)
			if (mMachineLoads.shouldMoveToMachine(machine) || moveEvenIfOtherMachineIsLoaded)
				{
				logTryingToMove_(comp, machine);

				mMachineLoads.taskIsMoving(comp, machine);

				mComputationsMoved++;

				mOnInitiateComputationMoved(
					InitiateComputationMove(comp, machine)
					);

				return true;
				}
		}

	return false;
	}

bool LocalSchedulerImplKernel::tryToMoveSomething_()
	{
	double t0 = curClock();
	if (mMoveCandidates.size() == 0)
		{
		for (auto compAndPri: mCurrentlyComputableComputationsByPriority.getKeyToValue())
			mMoveCandidates.push_back(compAndPri.first);

		for (long k = 0; k + 1 < mMoveCandidates.size(); k++)
			std::swap(
				mMoveCandidates[k], 
				mMoveCandidates[k + (mMoveCandidates.size() - k) * mRandomGenerator()]
				);
		}

	while (mMoveCandidates.size())
		{
		ComputationId comp = mMoveCandidates.back();
		mMoveCandidates.pop_back();

		if (mCurrentlyComputableComputations.hasKey(comp) && 
					mTryingToSplit.find(comp) == mTryingToSplit.end() && 
									!mMachineLoads.computationsMoving().hasKey(comp))
			{
			Nullable<ThreadGroup> owningGroup = mAllThreadGroups.containingThread(ThreadGroup::groupFor(comp));

			if (owningGroup)
				{
				const std::set<MachineId>& activeOn = mAllThreadGroupsActiveOn.getValues(*owningGroup);

				if (tryToMoveComputationToOneOf_(comp, activeOn, false))
					return true;
				}
			}
		}

	if (curClock() - t0 > .2)
		LOG_WARN << "Spent " << curClock() - t0 << " trying to split";

	return false;
	}

void LocalSchedulerImplKernel::tryToSplitSomething_()
	{
	Nullable<ComputationId> toSplit = searchForSplittableComputation_();

	if (toSplit)
		tryToSplitComputation_(*toSplit);
	}

void LocalSchedulerImplKernel::tryToSplitComputation_(ComputationId toSplit)
	{
	if (mTryingToSplit.find(toSplit) != mTryingToSplit.end())
		return;

	mTryingToSplit.insert(toSplit);

	const static double kMinTimeToComputeBeforeSplitting = .1;

	mTotalSplitAttempts++;

	sendSchedulerToComputationMessage(
		SchedulerToComputationMessage::Split(
			toSplit, 
			hash_type(), 
			mOwnMachineId,
			mOwnMachineId,
			kMinTimeToComputeBeforeSplitting
			)
		);

	if (mCurrentlyComputableComputations.hasKey(toSplit))
		{
		pair<long, ComputationPriority> curPriorityPair = 
			mCurrentlyComputableComputations.getValue(toSplit);

		lassert(curPriorityPair.first == 0);

		curPriorityPair.first = -1;

		mCurrentlyComputableComputations.set(toSplit, curPriorityPair);
		}
	}

}
}

