/***************************************************************************
    Copyright 2015 Ufora Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/

#include "CUDAExecutionContext.hppml"
#include "NativeCFGToPTX.hppml"
#include "../Core/Alignment.hpp"
#include "../Core/AlignmentManager.hpp"
#include "../Core/ExecutionContext.hppml"
#include "../Core/MemoryPool.hpp"
#include "../Core/ImplValContainerUtilities.hppml"
#include "../Native/NativeCFGTransforms/ConvertForGPUExecution.hppml"
#include "../Native/NativeCFGTransforms/Transforms.hppml"
#include "../Native/NativeCode.hppml"
#include "../Runtime.hppml"
#include "../TypedFora/JitCompiler/StaticInliner.hppml"
#include "../TypedFora/Converter.hppml"
#include "../TypedFora/ABI/SlottedForaValueArrayAppend.hppml"
#include "../TypedFora/ABI/VectorRecord.hpp"
#include "../TypedFora/ABI/VectorHandle.hpp"
#include "../TypedFora/TypedFora.hppml"
#include "../Reasoner/SimpleForwardReasoner.hppml"
#include "../VectorDataManager/PageletTree.hppml"
#include "../../core/Logging.hpp"
#include "../../core/threading/Queue.hpp"

#include <cuda.h>

using TypedFora::Abi::VectorHandle;
using TypedFora::Abi::VectorRecord;
using Fora::Interpreter::ExecutionContext;

constexpr uint8_t GPU_EXCEPTION_INDEX = 0xFF;

class CUDAExecutionContextInternalState {
public:

	CUDAExecutionContextInternalState() : mDeviceCount(0)
		{
		cudaDriverInitializer();
		cudaDeviceInitializer();
		}


	//initialize CUDA. single threaded, only happens once
	static	void	cudaDriverInitializer()
		{
		static boost::recursive_mutex	mMutex;

		static bool isInitialized = false;

		if (!isInitialized)
			{
			boost::recursive_mutex::scoped_lock	lock(mMutex);
			if (!isInitialized)
				{
				//initialize the driver
				throwOnError("Initialize cuda", cuInit(0));
				isInitialized = true;
				}
			}
		}

	static void throwOnError(std::string op, CUresult error)
		{
		if (error != CUDA_SUCCESS)
			{
			const char* ptr = 0;
			cuGetErrorString(error, &ptr);
			throw UnableToConvertToPTX("Couldn't " + op + ": " + std::string(ptr ? ptr : "") +
				". code=" + boost::lexical_cast<string>(error));
			}
		}

	void	cudaDeviceInitializer()
		{
		CUresult error = cuDeviceGetCount(&mDeviceCount);
		throwOnError("Get a device count", error);

		lassert_dump(mDeviceCount, "there are no CUDA-enabled devices");

		//take the first device
		mCuDevices.resize(mDeviceCount);
		mCuContexts.resize(mDeviceCount);

		for (long devID = 0; devID < mDeviceCount; devID++)
			{
			int major, minor;
			constexpr int DEVICE_NAME_MAX_LENGTH = 256;
			char deviceName[DEVICE_NAME_MAX_LENGTH];

			cuDeviceComputeCapability(&major, &minor, devID);
			cuDeviceGetName(deviceName, DEVICE_NAME_MAX_LENGTH, devID);

			LOG_INFO 	<< "CUDA Using Device " << devID
						<< ": \"" << deviceName
						<< "\" with Compute " << major << "." << minor
						<< "capability.\n"
						;

			// pick up device with zero ordinal (default, or devID)
			error = cuDeviceGet(&mCuDevices[devID], devID);

			lassert(error == CUDA_SUCCESS);

			// Create context
			error = cuCtxCreate(&mCuContexts[devID], 0, mCuDevices[devID]);
			lassert(error == CUDA_SUCCESS);
			}

		for (auto c: mCuContexts)
			mUnusedContexts.write(c);
		}

	string addLineNumbers(const string& str)
		{
		ostringstream outStr;

		uint32_t lastIx = 0;
		uint32_t lineNo = 1;

		for (long k = 0; k < str.size();k++)
			if (str[k] == '\n' || k == str.size() - 1)
				{
				outStr << lineNo << "   ";
				lineNo++;

				outStr << str.substr(lastIx, k - lastIx) << "\n";
				lastIx = k+1;
				}

		return outStr.str();
		}
	void addCudaModule(	const string& inModuleName,
						const string& inFunctionName,
						const string& inPTXCode)
		{
		for (auto context: mCuContexts)
			{
			cuCtxSetCurrent(context);

			const unsigned int jitNumOptions = 5;
			CUjit_option *jitOptions = new CUjit_option[jitNumOptions];
			void **jitOptVals = new void*[jitNumOptions];

			int bufferSize = 1024;
			char	jitLogBuffer[1024];
			char	jitErrorBuffer[1024];

			jitOptions[0] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;
			jitOptVals[0] = (void *)(size_t)bufferSize;
			jitOptions[1] = CU_JIT_INFO_LOG_BUFFER;
			jitOptVals[1] = jitLogBuffer;

			jitOptions[2] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;
			jitOptVals[2] = (void*)(size_t)bufferSize;
			jitOptions[3] = CU_JIT_ERROR_LOG_BUFFER;
			jitOptVals[3] = jitErrorBuffer;

			jitOptions[4] = CU_JIT_MAX_REGISTERS;
			int jitRegCount = 32;
			jitOptVals[4] = (void *)(size_t)jitRegCount;

			CUmodule	cuModule;
			CUresult error = cuModuleLoadDataEx(&cuModule, inPTXCode.c_str(),
							jitNumOptions, jitOptions, (void **)jitOptVals);

			if (error != CUDA_SUCCESS)
				{
				LOG_WARN << "CUDA FAILED TO COMPILE: " +
												string(jitLogBuffer) + "\n" +
												string(jitErrorBuffer) + "\n" +
												"\n\nPTX code = \n\n" +
												addLineNumbers(inPTXCode);

				lassert_dump(false, "CUDA FAILED TO COMPILE: " +
												string(jitLogBuffer) + "\n" +
												string(jitErrorBuffer) + "\n" +
												"\n\nPTX code = \n\n" +
												addLineNumbers(inPTXCode)
												);
				}

			CUfunction cuFun;

			// Get function handle from module
			error = cuModuleGetFunction(&cuFun, cuModule,
												inFunctionName.c_str());
			if (error != CUDA_SUCCESS)
				lassert_dump(false, "PTX code didn't define function " <<
											inFunctionName);
				{
				boost::mutex::scoped_lock lock(mMutex);
				mCuFunctionsByName[make_pair(context, inModuleName)] = cuFun;
				}
			}

		}


	CUfunction functionNameForModule(CUcontext context, std::string inModuleName)
		{
		boost::mutex::scoped_lock lock(mMutex);
		return mCuFunctionsByName[make_pair(context, inModuleName)];
		}

	void executeFunction(	const std::string&				inModuleName,
							void* 							inClosureData,
							uword_t							inClosureSize,
							uword_t							inN,
							void*							inSourceData,
							uword_t							inSourceElementSize,
							const std::vector<uint8_t*>&	outDestDataVectors,
							const std::vector<uword_t>&		inDestElementSize,
							void* 							outIndexData,
							uword_t							inIndexElementSize
							)
		{
		lassert(outDestDataVectors.size() > 0);
		lassert(inDestElementSize.size() > 0);
		lassert(outDestDataVectors.size() == inDestElementSize.size());

		uword_t outputTypesCount = outDestDataVectors.size();

		CUcontext contextToUse = mUnusedContexts.get();

		try {
			cuCtxSetCurrent(contextToUse);
			CUdeviceptr d_closureData, d_sourceData, d_indexData;
			std::vector<CUdeviceptr> d_destData(outputTypesCount);

			CUresult error;
			// Allocate and initialize device memory for closure data
			if (inClosureSize > 0)
				{
				lassert(inClosureData);
				error = cuMemAlloc(&d_closureData, inClosureSize);
				throwOnError("Allocate Closure Memory", error);
				error = cuMemcpyHtoD(d_closureData, inClosureData, inClosureSize);
				throwOnError("Copy CUDA memory from source to host", error);
				}

			// Allocate and initialize device memory for type-index data (output)
			if (outIndexData)
				{
				error = cuMemAlloc(&d_indexData, inIndexElementSize * inN);
				throwOnError("Allocate Source Memory", error);

				error = cuMemsetD8(d_indexData, 0xff, inIndexElementSize * inN);
				throwOnError("Set Memory to Zero", error);
				}

			// Allocate and initialize device memory for input data
			error = cuMemAlloc(&d_sourceData, inSourceElementSize * inN);
			throwOnError("Allocate Source Memory", error);

			error = cuMemcpyHtoD(d_sourceData, inSourceData, inSourceElementSize * inN);
			throwOnError("Copy CUDA memory from source to host", error);

			// Allocate device memory for output data
			for (uword_t k = 0; k < outputTypesCount; ++k)
				{
				ostringstream msg;
				msg << "Allocate Destination Memory: "
					<< inDestElementSize[k] << " * " << inN;
				LOG_DEBUG << msg.str();
				error = cuMemAlloc(&d_destData[k], inDestElementSize[k] * inN);
				throwOnError(msg.str(), error);
				}

			// Grid/Block configuration
			int threadsPerBlock = 1024;
			int blocksPerGrid   = (inN + threadsPerBlock - 1) / threadsPerBlock;

			uword_t kk = 0;
			uword_t extraArgs = 2; // inN and sourceDataPtr
			if (inClosureSize >0)
				++extraArgs;
			if (outIndexData)
				++extraArgs;
			uword_t argCount = outputTypesCount + extraArgs;
			void **args = (void**) malloc(sizeof(void*) * (argCount));
			if (inClosureSize > 0)
				args[kk++] = &d_closureData;
			args[kk++] = &inN;
			args[kk++] = &d_sourceData;
			if (outIndexData)
				args[kk++] = &d_indexData;
			for (uword_t k = 0; k < outputTypesCount; ++k)
				{
				lassert(kk < argCount);
				args[kk++] = &d_destData[k];
				}
			lassert(kk == argCount);

			// Launch the CUDA kernel
			error = cuLaunchKernel( functionNameForModule(contextToUse, inModuleName),
									blocksPerGrid, 1, 1,
									threadsPerBlock, 1, 1,
									0,
									NULL, args, NULL);

			lassert_dump(error == CUDA_SUCCESS, "UNKNOWN CUDA ERROR");
			//copy to target
			if (outIndexData)
				{
				error = cuMemcpyDtoH(outIndexData, d_indexData, inIndexElementSize * inN);
				throwOnError("Copy Index to Destination Memory", error);
				}
			for (uword_t k = 0; k < outputTypesCount; ++k)
				{
				error = cuMemcpyDtoH(	outDestDataVectors[k],
										d_destData[k],
										inDestElementSize[k] * inN
										);
				throwOnError("Copy output to Destination Memory", error);
				}
			//free cuda memory
			if (inClosureSize > 0)
				cuMemFree(d_closureData);
			if (outIndexData)
				cuMemFree(d_indexData);
			cuMemFree(d_sourceData);
			for (uword_t k = 0; k < outputTypesCount; ++k)
				cuMemFree(d_destData[k]);
			free(args);

			mUnusedContexts.write(contextToUse);
			}
		catch(...)
			{
			mUnusedContexts.write(contextToUse);
			// FIXME: we are not freeing the cuda allocated memory properly
			throw;
			}
		}
private:
	int mDeviceCount;

	Queue<CUcontext> mUnusedContexts;

	std::vector<CUdevice> mCuDevices;

	std::vector<CUcontext> mCuContexts;

	boost::mutex mMutex;

	map<pair<CUcontext, string>, CUfunction> mCuFunctionsByName;
};

CUDAExecutionContext::CUDAExecutionContext() :
		mCUDAState(new CUDAExecutionContextInternalState())
	{
	}

bool	CUDAExecutionContext::isCUDASupportCompiledIn(void)
	{
	return true;
	}

void	CUDAExecutionContext::define(
						const std::string& inKernelName,
						const NativeCFG& inCFG,
						const Type& inInputType,
						const std::vector<Type>& inOutputTypes
						)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	// TODO:: assert that inCFG.returnTypes() match inOutputTypes
	string 	ptxDefinition =
		computePTXVectorApplyKernelFromNativeCFG(inCFG, inKernelName);

	mCUDAState->addCudaModule(inKernelName, inKernelName, ptxDefinition);

	mNativeKernelsByName[inKernelName] = inCFG;
	mPTXKernelFunctionNames[inKernelName] = inKernelName;
	mPTXKernelsByName[inKernelName] = ptxDefinition;
	mInputOutputTypesByName[inKernelName] = make_pair(inInputType, inOutputTypes);
	}

ImplValContainer    createFORAVector(
						uint8_t* indexData,
						uword_t  indexAlignedSize, // we can also pass the Type if necessary
						const std::vector<Type>& outputElementTypes,
						const std::vector<uint8_t*> alignedDataVectors,
						uint64_t count,
						MemoryPool* inPool
						)
	{
	if (!count || outputElementTypes.size() == 0 || alignedDataVectors.size() == 0)
		return ImplValContainerUtilities::createVector(VectorRecord());

	lassert(inPool);

	TypedFora::Abi::ForaValueArray* array =
		TypedFora::Abi::ForaValueArray::Empty(inPool);

	std::set<JOV> jovSet;
	std::vector<uint8_t*> slots(count);
	std::vector<JOV> jovs(count);
	if (indexData)
		{
		LOG_DEBUG << "createFORAVector: 2";
		LOG_DEBUG << "createFORAVector: 3";
		for (uword_t k = 0; k < count; ++k)
			{
			LOG_DEBUG << "createFORAVector: 3.0";
			uint8_t index = indexData[k];
			if (index == GPU_EXCEPTION_INDEX)
				return ImplValContainerUtilities::createVector(VectorRecord());

			LOG_DEBUG << "createFORAVector: 3.1: index = " << (unsigned int)index;
			lassert_dump(index < outputElementTypes.size(),
					"index = " << (long)index << ", elmtTypes = " << outputElementTypes.size());
			const Type& t = outputElementTypes[index];
			LOG_DEBUG << "createFORAVector: 3.2";
			slots[k] = alignedDataVectors[index] + (k * t.alignedSize());
			LOG_DEBUG << "createFORAVector: 3.3";
			jovs[k] = JOV::OfType(t);
			jovSet.insert(jovs[k]);
			}
		}
	else
		jovSet.insert(JOV::OfType(outputElementTypes[0]));

	lassert(jovSet.size() > 0);
	if (jovSet.size() == 1)
		{
		const JOV& objectType = *jovSet.begin();
		uint8_t* alignedData;
		if (indexData)
			alignedData = alignedDataVectors[indexData[0]];
		else
			alignedData = alignedDataVectors[0];
		TypedFora::Abi::PackedForaValues vals = array->appendUninitialized(objectType, count);
		if (objectType.type()->alignedSize() != objectType.type()->size())
			{
			uint8_t* packedData  = vals.data();
			for (int i = 0; i < count; ++i)
				copyAlignedToPacked(*objectType.type(), (uint8_t*)alignedData, packedData);
			}
		else
			memcpy(vals.data(), alignedData, objectType.type()->size() * count);
		}
	else
		{
		TypedFora::Abi::slottedAppend(slots, jovs, jovSet, array);
		}

	VectorRecord vector(
		inPool->construct<VectorHandle>(
			Fora::BigVectorId(),
			Fora::PageletTreePtr(),
			array,
			inPool,
			ExecutionContext::currentExecutionContext()->newVectorHash()
			)
		);
	LOG_DEBUG << "createFORAVector: 6";
	return ImplValContainerUtilities::createVector(vector);
	}


ImplValContainer	CUDAExecutionContext::executeKernel(
						const std::string&	inKernelName,
						ImplValContainer	inApplyObject,
						ImplValContainer	inSourceVector
						)
	{
	LOG_DEBUG << "executeKernel(2): 1";

	Type closureType = inApplyObject.type();
	Type inputElementType;
	const std::vector<Type>* outputElementTypes;
	bool mayThrowException = false;

		{
		boost::recursive_mutex::scoped_lock lock(mMutex);

		//verify that this kernel exists
		lassert_dump(
				mInputOutputTypesByName.find(inKernelName) != mInputOutputTypesByName.end(),
				"used kernel " << inKernelName << " without defining it."
				);
		lassert_dump(
				mMayThrowException.find(inKernelName) != mMayThrowException.end(),
				"used kernel " << inKernelName << " without defining mMayThrowException for it."
				);
		inputElementType = mInputOutputTypesByName[inKernelName].first;
		outputElementTypes = &mInputOutputTypesByName[inKernelName].second;
		mayThrowException = mMayThrowException[inKernelName];
		lassert(outputElementTypes);
		}

	lassert(inApplyObject.type().isClass());
	lassert(inSourceVector.type().isVector());
	LOG_DEBUG << "executeKernel(2): 2";

	TypedFora::Abi::VectorRecord sourceVectorHandle = inSourceVector.cast<TypedFora::Abi::VectorRecord>();

	lassert(sourceVectorHandle.size() &&
			sourceVectorHandle.jor().size() == 1 &&
			sourceVectorHandle.jor()[0].type());

	lassert_dump(
		*sourceVectorHandle.jor()[0].type() == inputElementType,
		"passed kernel " << inKernelName << " vector with elements of type "
				<< prettyPrintString(*sourceVectorHandle.jor()[0].type())
				<< " but expected" << prettyPrintString(inputElementType)
		);

	//bail if there are no elements
	if (!sourceVectorHandle.size())
		return ImplValContainer(
			CSTValue::blankOf(
				Type::Vector()
				)
			);

	uword_t elementCount = sourceVectorHandle.size();
	lassert(ExecutionContext::currentExecutionContext());
	MemoryPool* pool = ExecutionContext::currentExecutionContext()->getMemoryPool();
	LOG_DEBUG << "executeKernel(2): 3";

	//create a aligned vectors
	AlignmentManager alignMgr;
	LOG_DEBUG << "executeKernel(2): 3.0";
	uint8_t* rawClosureData = alignMgr.getHandleToAlignedData(inApplyObject);
	LOG_DEBUG << "executeKernel(2): 3.1";
	uint8_t* rawInVecData   = alignMgr.getHandleToAlignedData(inSourceVector);
	LOG_DEBUG << "executeKernel(2): 3.2";
	if (!rawInVecData || (!rawClosureData && inApplyObject.type().size() > 0))
		return ImplValContainer(CSTValue::blankOf(Type::Vector()));
	LOG_DEBUG << "executeKernel(2): 4";

	uword_t outputElementTypesCount = outputElementTypes->size();
	lassert_dump(outputElementTypesCount < GPU_EXCEPTION_INDEX, // reserve value 0xFF to flag exceptions
			"Too many output types for 8bit unsigned integer to encode");
	Type indexElementType = Type::Integer(8, false);

	uint8_t* rawOutVecIndex = nullptr;
	if (outputElementTypesCount > 1 || mayThrowException)
		rawOutVecIndex = alignMgr.allocateAlignedData(indexElementType, elementCount);

	std::vector<uint8_t*> rawOutVecDataVectors(outputElementTypesCount);
	std::vector<uword_t> outputElementTypeSizes(outputElementTypesCount);
	for (uword_t k = 0; k < outputElementTypesCount; ++k)
		{
		const Type& t = (*outputElementTypes)[k];
		uint8_t* ptr = alignMgr.allocateAlignedData(t, elementCount);
		if (!ptr)
			return ImplValContainer(CSTValue::blankOf(Type::Vector()));
		rawOutVecDataVectors[k] = ptr;
		outputElementTypeSizes[k] = t.alignedSize();
		}
	LOG_DEBUG << "executeKernel(2): 5";

	lassert(sourceVectorHandle.dataPtr()->unpagedValues());
	lassert(sourceVectorHandle.dataPtr()->unpagedValues()->size() == elementCount);

	LOG_DEBUG << "executeKernel(2): 6";
	mCUDAState->executeFunction(
		inKernelName,
		rawClosureData,
		closureType.alignedSize(),
		elementCount,
		rawInVecData,
		inputElementType.alignedSize(),
		rawOutVecDataVectors,
		outputElementTypeSizes,
		rawOutVecIndex,
		indexElementType.alignedSize()
		);
	LOG_DEBUG << "executeKernel(2): 7";
	auto res = createFORAVector(
					rawOutVecIndex,
					indexElementType.alignedSize(),
					*outputElementTypes,
					rawOutVecDataVectors,
					elementCount,
					pool);
	LOG_DEBUG << "executeKernel(2): 8";

	return res;
	}


class StripCallbackDataTransformer {
public:
	template<class T>
	Nullable<T> processDown(const T& in, bool& out) const
		{
		return null();
		}
	template<class T>
	Nullable<T> processUp(const T& in) const
		{
		return null();
		}
	//strip out the callback variables and replace with 'nothing'
	//won't be a problem unless they get used
	NativeVariable 		processDown(const NativeVariable& t, bool& cont) const
		{
		if (isRuntimeCallbackType(t.type()))
			return NativeVariable(t.varID(), NativeType::Nothing());
		return t;
		}
	NativeExpression	processDown(const NativeExpression& t, bool& cont) const
		{
		@match NativeExpression(t)
			-|	Tagged(_, Interrupt()) ->> {
				return  NativeExpression::Nothing();
				}
			-|	Tagged(_, KickToInterpreterCheck()) ->> {
				return  NativeExpression::Nothing();
				}
			-|	x ->> {
				return  x;
				}
			;
		}
};


ImplValContainer	CUDAExecutionContext::executeKernel(
						ImplValContainer	inApplyObject,
						ImplValContainer	inSourceVector
						)
	{
	Type vecElementType = *inSourceVector.cast<VectorRecord>().jor()[0].type();

	if (!vecElementType.isPOD())
		{
		throw UnableToConvertToPTX(
			prettyPrintString(vecElementType) + " isn't POD");
		}

	JudgmentOnValue	funJOV =
		JudgmentOnValue::Constant(inApplyObject.getReference()).relaxedJOV();

	string kernelName = "CUDA_" + hashToString(funJOV.hash() + vecElementType.hash());

		{
		boost::recursive_mutex::scoped_lock lock(mMutex);

		if (mNativeKernelsByName.find(kernelName) == mNativeKernelsByName.end())
			{
			ImmutableTreeVector<JudgmentOnValue> signatureJOVs =
				emptyTreeVec() +
					funJOV +
					JudgmentOnValue::Constant(CSTValue(Symbol::Call())) +
					JudgmentOnValue::OfType(vecElementType)
				;

			PolymorphicSharedPtr<Fora::SimpleForwardReasoner> reasoner(
				new Fora::SimpleForwardReasoner(
					Runtime::getRuntime().getTypedForaCompiler(),
		            Runtime::getRuntime().getInstructionGraph(),
		            Runtime::getRuntime().getAxioms()
		            )
				);

			auto frame = reasoner->reasonAboutApply(JOVT::Unnamed(signatureJOVs));

			if (frame->exits().resultPart().size() < 1 )
				{
				ostringstream msg;
				msg << "Code returns zero types ("
						<< frame->exits().resultPart().size() << ").";
				throw UnableToConvertToPTX(msg.str());
				}
			for (auto& jov : frame->exits().resultPart().vals())
				LOG_DEBUG << "result jov : " << prettyPrintString(jov);

			auto res = reasoner->compileEntrypointForApply(JOVT::Unnamed(signatureJOVs));

			if (!res)
				throw UnableToConvertToPTX(
					"Reasoning failed to converge."
					);

			//the target instruction might have different JOVS
			//than the signature, because the axiom might be
			//a 'class apply' in which case the instruction
			//will have all of the class object's members
			//as arguments as well. These will all be constants
			//in this case, and will be removed
			//from the NativeCFG because they're all passed as
			//'none'
			ImmutableTreeVector<JudgmentOnValue>
				targetInstructionJOVs = signatureJOVs;

			//if the joa() has multiple exit points that are compatible,
			//we need to wrap them up into a single typed value
			bool	needsReturnTypeModification = false;
			TypedFora::Converter converter;
			LOG_DEBUG << "executeKernel: 1";

			NativeCFG cfg =
					converter.convertCallable(
							Runtime::getRuntime().getTypedForaCompiler()->getDefinition(res->second));

			lassert(res->first == TypedFora::BlockID::entry());

			cfg = transform(
					NativeCFGTransforms::convertForGpuExecution(cfg, frame->exits().resultPart().size()),
					StripCallbackDataTransformer());
			LOG_DEBUG << "executeKernel: 2";

			while (cfg.externalBranches().size())
				{
				string nameOfSubbranch = cfg.externalBranches()[0];
				if (Runtime::getRuntime().getTypedForaCompiler()->
						getMutuallyRecursiveFunctions(nameOfSubbranch).size())
					{
					throw UnableToConvertToPTX("contains recursion");
					}
				NativeCFG cfgToInline =
						converter.convertCallable(
								Runtime::getRuntime().getTypedForaCompiler()
									->getDefinition(nameOfSubbranch));
				cfg = NativeCFGTransforms::inlineCFG(cfg,
					transform(
						NativeCFGTransforms::convertForGpuExecution(
							cfgToInline,
							cfgToInline.returnTypes().size()
							),
						StripCallbackDataTransformer()
						),
					nameOfSubbranch
					);
				}
			LOG_DEBUG << "executeKernel: 3";
			try {
				mMayThrowException[kernelName] = true;
				//a lot simpler when all variables are unique
				LOG_DEBUG << "executeKernel: 4";
				LOG_DEBUG << "CFG:\n\n" << prettyPrintString(cfg);
				cfg = NativeCFGTransforms::optimize(cfg, Runtime::getRuntime().getConfig());
				LOG_DEBUG << "executeKernel: 5";
				cfg = NativeCFGTransforms::renameVariables(cfg);
				LOG_DEBUG << "executeKernel: 6";

				std::vector<Type> outputTypes(frame->exits().resultPart().size());
					{
					long k = 0;
					for (auto& jov : frame->exits().resultPart().vals())
						{
						Nullable<Type> t = jov.type();
						lassert_dump(
								t,
								"return type at index " << k << " of CUDA kernel '"
									<< kernelName << "' is missing");
						outputTypes[k] = *t;
						LOG_DEBUG << " output_type[" << k << "] = " << prettyPrintString(*t);
						++k;
						}
					}

				define(kernelName, cfg, vecElementType, outputTypes);
				LOG_DEBUG << "executeKernel: 8";
				}
			catch(std::logic_error e)
				{
				throw UnableToConvertToPTX("internal error: " +
					string(e.what()));
				}
			}
		}
	LOG_DEBUG << "executeKernel: 9";
	return executeKernel(kernelName, inApplyObject, inSourceVector);
	}
